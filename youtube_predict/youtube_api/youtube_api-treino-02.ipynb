{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#insira sua chave da API do Youtube\n",
    "API_KEY = \"AIzaSyB8MZSRAlMuLkjLdqqU4G6THN4l7mnEUls\"\n",
    "\n",
    "# URL base da API\n",
    "BASE_URL = \"https://www.googleapis.com/youtube/v3\"\n",
    "\n",
    "# função para buscar informaçoes do canal\n",
    "def get_channel_info(channel_id):\n",
    "    url = f\"{BASE_URL}/channels?part=snippet,statistics&id={channel_id}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "# Teste com ID de um canal\n",
    "CHANNEL_ID = \"UC9cz05xObaFpB8U72t73IFA\"\n",
    "info = get_channel_info(CHANNEL_ID)\n",
    "\n",
    "# Exibir JSON formatado\n",
    "print(json.dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(channel_id):\n",
    "    url = f\"{BASE_URL}/search?key={API_KEY}&channelId={channel_id}&part=id&order=date&maxResults=50\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erro na requisição: {response.status_code}, {response.text}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    \n",
    "    video_ids = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "            video_ids.append(item[\"id\"][\"videoId\"])\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Buscar os vídeos do canal\n",
    "CHANNEL_ID = \"UC9cz05xObaFpB8U72t73IFA\"\n",
    "video_ids = get_video_ids(CHANNEL_ID)\n",
    "\n",
    "# Exibir os primeiros 5 IDs para conferir\n",
    "print(video_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(video_ids):\n",
    "    video_data = []\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        url = f\"{BASE_URL}/videos?part=snippet,statistics,contentDetails&id={video_id}&key={API_KEY}\"\n",
    "        response = requests.get(url).json()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            video_info = {\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"description\": item[\"snippet\"][\"description\"],\n",
    "                \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                \"views\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                \"likes\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                \"comments\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                \"duration\": item[\"contentDetails\"][\"duration\"],\n",
    "            }\n",
    "            video_data.append(video_info)\n",
    "    \n",
    "    return video_data\n",
    "\n",
    "# Buscar informações dos vídeos\n",
    "video_details = get_video_details(video_ids)\n",
    "\n",
    "# Exibir as informações do primeiro vídeo como teste\n",
    "video_details[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criar DataFrame com os dados coletados\n",
    "df_videos = pd.DataFrame(video_details)\n",
    "\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import isodate\n",
    "\n",
    "# Converter a data de publicação para formato datetime\n",
    "df_videos[\"published_at\"] = pd.to_datetime(df_videos[\"published_at\"])\n",
    "\n",
    "# Converter a duração para segundos\n",
    "def parse_duration(duration):\n",
    "    return isodate.parse_duration(duration).total_seconds()\n",
    "\n",
    "df_videos[\"duration_sec\"] = df_videos[\"duration\"].apply(parse_duration)\n",
    "\n",
    "# Exibir os dados convertidos\n",
    "df_videos[[\"published_at\", \"duration\", \"duration_sec\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar colunas para ano, mês, dia e dia da semana\n",
    "df_videos[\"year\"] = df_videos[\"published_at\"].dt.year\n",
    "df_videos[\"month\"] = df_videos[\"published_at\"].dt.month\n",
    "df_videos[\"day\"] = df_videos[\"published_at\"].dt.day\n",
    "df_videos[\"weekday\"] = df_videos[\"published_at\"].dt.day_name()\n",
    "\n",
    "# Criar colunas para métricas de engajamento\n",
    "df_videos[\"likes_per_view\"] = df_videos[\"likes\"] / df_videos[\"views\"]\n",
    "df_videos[\"comments_per_view\"] = df_videos[\"comments\"] / df_videos[\"views\"]\n",
    "\n",
    "# Exibir os dados processados\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagem de vídeos por dia da semana\n",
    "df_videos[\"weekday\"].value_counts().plot(kind=\"bar\", title=\"Vídeos Postados por Dia da Semana\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_videos[\"duration_sec\"] / 60, df_videos[\"views\"], alpha=0.5)\n",
    "plt.xlabel(\"Duração (minutos)\")\n",
    "plt.ylabel(\"Visualizações\")\n",
    "plt.title(\"Relação entre Duração do Vídeo e Visualizações\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.plot(x=\"likes_per_view\", y=\"comments_per_view\", kind=\"scatter\", title=\"Correlação entre Likes e Comentários\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos[\"title_word_count\"] = df_videos[\"title\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.scatter(df_videos[\"title_word_count\"], df_videos[\"views\"], alpha=0.5)\n",
    "plt.xlabel(\"Número de Palavras no Título\")\n",
    "plt.ylabel(\"Visualizações\")\n",
    "plt.title(\"Relação entre Número de Palavras no Título e Visualizações\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Lista de palavras irrelevantes (pode ser ajustada conforme necessário)\n",
    "stopwords = {\"a\", \"o\", \"os\", \"as\", \"de\", \"da\", \"do\", \"dos\", \"das\", \"em\", \"para\", \n",
    "             \"por\", \"com\", \"um\", \"uma\", \"uns\", \"umas\", \"e\", \"ou\", \"se\", \"que\", \n",
    "             \"é\", \"na\", \"no\", \"nas\", \"nos\", \"como\", \"mais\", \"menos\", \"muito\", \n",
    "             \"pouco\", \"ser\", \"ter\", \"vai\", \"tá\", \"to\", \"vou\"}\n",
    "\n",
    "# Função para limpar o texto e contar palavras relevantes\n",
    "def contar_palavras_relevantes(textos):\n",
    "    palavras = []\n",
    "    for texto in textos:\n",
    "        palavras.extend(\n",
    "            word for word in re.findall(r'\\b\\w+\\b', texto.lower()) if word not in stopwords\n",
    "        )  \n",
    "    return Counter(palavras)\n",
    "\n",
    "# Contar palavras relevantes nos títulos dos vídeos mais vistos (top 20%)\n",
    "top_videos = df_videos.nlargest(int(len(df_videos) * 0.2), \"views\")\n",
    "palavras_relevantes = contar_palavras_relevantes(top_videos[\"title\"])\n",
    "\n",
    "# Exibir as 10 palavras mais frequentes nos títulos dos vídeos de maior sucesso\n",
    "palavras_relevantes.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palavras irrelevantes (stopwords)\n",
    "stopwords = {\"a\", \"o\", \"os\", \"as\", \"de\", \"da\", \"do\", \"dos\", \"das\", \"em\", \"para\", \n",
    "             \"por\", \"com\", \"um\", \"uma\", \"uns\", \"umas\", \"e\", \"ou\", \"se\", \"que\", \n",
    "             \"é\", \"na\", \"no\", \"nas\", \"nos\", \"como\", \"mais\", \"menos\", \"muito\", \n",
    "             \"pouco\", \"ser\", \"ter\", \"vai\", \"tá\", \"to\", \"vou\", \"sobre\", \"essa\", \"esse\",\n",
    "             \"isso\", \"isso\", \"todo\", \"toda\", \"todos\", \"todas\", \"neste\", \"nesta\", \"nesse\"}\n",
    "\n",
    "# Função para limpar o texto das descrições\n",
    "def limpar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Remover URLs (https:// ou www.)\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "    # Remover hashtags e menções (@usuário)\n",
    "    texto = re.sub(r\"#\\w+|@\\w+\", \"\", texto)\n",
    "    # Remover caracteres especiais que não são palavras\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)\n",
    "    return texto\n",
    "\n",
    "# Função para contar palavras relevantes em descrições\n",
    "def contar_palavras_relevantes_desc(textos):\n",
    "    palavras = []\n",
    "    for texto in textos:\n",
    "        texto_limpo = limpar_texto(texto)\n",
    "        palavras.extend(\n",
    "            word for word in re.findall(r'\\b\\w+\\b', texto_limpo) if word not in stopwords\n",
    "        )\n",
    "    return Counter(palavras)\n",
    "\n",
    "# Contar palavras relevantes nas descrições dos vídeos mais vistos (top 20%)\n",
    "palavras_relevantes_desc = contar_palavras_relevantes_desc(top_videos[\"description\"])\n",
    "\n",
    "# Exibir as 10 palavras mais frequentes nas descrições dos vídeos de maior sucesso\n",
    "palavras_relevantes_desc.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configurar o TF-IDF\n",
    "vectorizer_title = TfidfVectorizer(max_features=100)\n",
    "vectorizer_desc = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Transformar títulos e descrições\n",
    "title_tfidf = vectorizer_title.fit_transform(df_videos[\"title\"])\n",
    "desc_tfidf = vectorizer_desc.fit_transform(df_videos[\"description\"])\n",
    "\n",
    "# Converter para DataFrame\n",
    "df_title_tfidf = pd.DataFrame(title_tfidf.toarray(), columns=vectorizer_title.get_feature_names_out())\n",
    "df_desc_tfidf = pd.DataFrame(desc_tfidf.toarray(), columns=vectorizer_desc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar TF-IDF com o DataFrame principal\n",
    "df_model = pd.concat([df_videos, df_title_tfidf, df_desc_tfidf], axis=1)\n",
    "\n",
    "# Selecionar as features que vamos usar para o modelo\n",
    "features = df_model.drop(columns=[\"video_id\", \"title\", \"description\", \"published_at\", \"duration\", \"views\"])\n",
    "target = df_model[\"views\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Treinar o modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest - MSE: {mse_rf}\")\n",
    "print(f\"Random Forest - R² Score: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"R² Score após normalização: {r2_score(y_test, y_pred_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converter X_train e X_test para arrays NumPy\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Verificar se a conversão funcionou\n",
    "print(type(X_train), X_train.shape)\n",
    "print(type(X_test), X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Criar o modelo XGBoost\n",
    "model_xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost - MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost - R² Score: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definir o modelo base\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definir os hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Rodar o GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring=\"r2\", verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhor modelo encontrado\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Avaliar o modelo otimizado\n",
    "y_pred_rf_best = best_rf.predict(X_test)\n",
    "mse_rf_best = mean_squared_error(y_test, y_pred_rf_best)\n",
    "r2_rf_best = r2_score(y_test, y_pred_rf_best)\n",
    "\n",
    "print(f\"Melhor Random Forest - MSE: {mse_rf_best}\")\n",
    "print(f\"Melhor Random Forest - R² Score: {r2_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = {\n",
    "    \"Linear Regression\": r2,\n",
    "    \"Random Forest\": r2_rf,\n",
    "    \"Random Forest (Tuned)\": r2_rf_best,\n",
    "    \"XGBoost\": r2_xgb\n",
    "}\n",
    "\n",
    "for model, score in models_results.items():\n",
    "    print(f\"{model}: R² Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o RMSE para cada modelo\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "rmse_rf_best = np.sqrt(mse_rf_best)\n",
    "\n",
    "print(f\"XGBoost - RMSE: {rmse_xgb}\")\n",
    "print(f\"Random Forest (Otimizado) - RMSE: {rmse_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ajustando hiperparâmetros manualmente\n",
    "model_xgb_tuned = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=8, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "model_xgb_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_xgb_tuned = model_xgb_tuned.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse_xgb_tuned = mean_squared_error(y_test, y_pred_xgb_tuned)\n",
    "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "print(f\"XGBoost (Otimizado) - MSE: {mse_xgb_tuned}\")\n",
    "print(f\"XGBoost (Otimizado) - R² Score: {r2_xgb_tuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemplo de novo vídeo para previsão\n",
    "novo_video = pd.DataFrame({\n",
    "    \"title\": [\"Treino de Hipertrofia para Iniciantes\"],\n",
    "    \"description\": [\"Descubra o melhor treino para ganhar massa muscular! Assista agora e veja os melhores exercícios.\"],\n",
    "    \"duration_sec\": [600],  # Duração do vídeo em segundos (exemplo: 10 minutos)\n",
    "    \"year\": [2025],\n",
    "    \"month\": [1],\n",
    "    \"day\": [31],\n",
    "    \"weekday_Monday\": [0],\n",
    "    \"weekday_Tuesday\": [0],\n",
    "    \"weekday_Wednesday\": [1],  # Supondo que será postado numa quarta-feira\n",
    "    \"weekday_Thursday\": [0],\n",
    "    \"weekday_Friday\": [0],\n",
    "    \"weekday_Saturday\": [0],\n",
    "    \"weekday_Sunday\": [0]\n",
    "})\n",
    "\n",
    "# Exibir o DataFrame do novo vídeo\n",
    "novo_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o título e a descrição do novo vídeo para TF-IDF\n",
    "novo_video_title_tfidf = vectorizer_title.transform(novo_video[\"title\"]).toarray()\n",
    "novo_video_desc_tfidf = vectorizer_desc.transform(novo_video[\"description\"]).toarray()\n",
    "\n",
    "# Converter para DataFrame\n",
    "df_title_tfidf_novo = pd.DataFrame(novo_video_title_tfidf, columns=vectorizer_title.get_feature_names_out())\n",
    "df_desc_tfidf_novo = pd.DataFrame(novo_video_desc_tfidf, columns=vectorizer_desc.get_feature_names_out())\n",
    "\n",
    "# Juntar os dados numéricos com os TF-IDF\n",
    "novo_video_model = pd.concat([novo_video.drop(columns=[\"title\", \"description\"]), df_title_tfidf_novo, df_desc_tfidf_novo], axis=1)\n",
    "\n",
    "# Exibir o DataFrame formatado para o modelo\n",
    "novo_video_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train)  # Substitui NaNs por 0 diretamente no array NumPy\n",
    "X_test = np.nan_to_num(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#insira sua chave da API do Youtube\n",
    "API_KEY = \"AIzaSyB8MZSRAlMuLkjLdqqU4G6THN4l7mnEUls\"\n",
    "\n",
    "# URL base da API\n",
    "BASE_URL = \"https://www.googleapis.com/youtube/v3\"\n",
    "\n",
    "# função para buscar informaçoes do canal\n",
    "def get_channel_info(channel_id):\n",
    "    url = f\"{BASE_URL}/channels?part=snippet,statistics&id={channel_id}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "# Teste com ID de um canal\n",
    "CHANNEL_ID = \"UC9cz05xObaFpB8U72t73IFA\"\n",
    "info = get_channel_info(CHANNEL_ID)\n",
    "\n",
    "# Exibir JSON formatado\n",
    "print(json.dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(channel_id):\n",
    "    url = f\"{BASE_URL}/search?key={API_KEY}&channelId={channel_id}&part=id&order=date&maxResults=50\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erro na requisição: {response.status_code}, {response.text}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    \n",
    "    video_ids = []\n",
    "    for item in data.get(\"items\", []):\n",
    "        if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "            video_ids.append(item[\"id\"][\"videoId\"])\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Buscar os vídeos do canal\n",
    "CHANNEL_ID = \"UC9cz05xObaFpB8U72t73IFA\"\n",
    "video_ids = get_video_ids(CHANNEL_ID)\n",
    "\n",
    "# Exibir os primeiros 5 IDs para conferir\n",
    "print(video_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(video_ids):\n",
    "    video_data = []\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        url = f\"{BASE_URL}/videos?part=snippet,statistics,contentDetails&id={video_id}&key={API_KEY}\"\n",
    "        response = requests.get(url).json()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            video_info = {\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"description\": item[\"snippet\"][\"description\"],\n",
    "                \"published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "                \"views\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                \"likes\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                \"comments\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                \"duration\": item[\"contentDetails\"][\"duration\"]\n",
    "            }\n",
    "            video_data.append(video_info)\n",
    "    \n",
    "    return video_data\n",
    "\n",
    "# Buscar informações dos vídeos\n",
    "video_details = get_video_details(video_ids)\n",
    "\n",
    "# Exibir as informações do primeiro vídeo como teste\n",
    "video_details[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criar DataFrame com os dados coletados\n",
    "df_videos = pd.DataFrame(video_details)\n",
    "\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import isodate\n",
    "\n",
    "# Converter a data de publicação para formato datetime\n",
    "df_videos[\"published_at\"] = pd.to_datetime(df_videos[\"published_at\"])\n",
    "\n",
    "# Converter a duração para segundos\n",
    "def parse_duration(duration):\n",
    "    return isodate.parse_duration(duration).total_seconds()\n",
    "\n",
    "df_videos[\"duration_sec\"] = df_videos[\"duration\"].apply(parse_duration)\n",
    "\n",
    "# Exibir os dados convertidos\n",
    "df_videos[[\"published_at\", \"duration\", \"duration_sec\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar colunas para ano, mês, dia e dia da semana\n",
    "df_videos[\"year\"] = df_videos[\"published_at\"].dt.year\n",
    "df_videos[\"month\"] = df_videos[\"published_at\"].dt.month\n",
    "df_videos[\"day\"] = df_videos[\"published_at\"].dt.day\n",
    "df_videos[\"weekday\"] = df_videos[\"published_at\"].dt.day_name()\n",
    "\n",
    "# Criar colunas para métricas de engajamento\n",
    "df_videos[\"likes_per_view\"] = df_videos[\"likes\"] / df_videos[\"views\"]\n",
    "df_videos[\"comments_per_view\"] = df_videos[\"comments\"] / df_videos[\"views\"]\n",
    "\n",
    "# Exibir os dados processados\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagem de vídeos por dia da semana\n",
    "df_videos[\"weekday\"].value_counts().plot(kind=\"bar\", title=\"Vídeos Postados por Dia da Semana\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_videos[\"duration_sec\"] / 60, df_videos[\"views\"], alpha=0.5)\n",
    "plt.xlabel(\"Duração (minutos)\")\n",
    "plt.ylabel(\"Visualizações\")\n",
    "plt.title(\"Relação entre Duração do Vídeo e Visualizações\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.plot(x=\"likes_per_view\", y=\"comments_per_view\", kind=\"scatter\", title=\"Correlação entre Likes e Comentários\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos[\"title_word_count\"] = df_videos[\"title\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.scatter(df_videos[\"title_word_count\"], df_videos[\"views\"], alpha=0.5)\n",
    "plt.xlabel(\"Número de Palavras no Título\")\n",
    "plt.ylabel(\"Visualizações\")\n",
    "plt.title(\"Relação entre Número de Palavras no Título e Visualizações\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Lista de palavras irrelevantes (pode ser ajustada conforme necessário)\n",
    "stopwords = {\"a\", \"o\", \"os\", \"as\", \"de\", \"da\", \"do\", \"dos\", \"das\", \"em\", \"para\", \n",
    "             \"por\", \"com\", \"um\", \"uma\", \"uns\", \"umas\", \"e\", \"ou\", \"se\", \"que\", \n",
    "             \"é\", \"na\", \"no\", \"nas\", \"nos\", \"como\", \"mais\", \"menos\", \"muito\", \n",
    "             \"pouco\", \"ser\", \"ter\", \"vai\", \"tá\", \"to\", \"vou\"}\n",
    "\n",
    "# Função para limpar o texto e contar palavras relevantes\n",
    "def contar_palavras_relevantes(textos):\n",
    "    palavras = []\n",
    "    for texto in textos:\n",
    "        palavras.extend(\n",
    "            word for word in re.findall(r'\\b\\w+\\b', texto.lower()) if word not in stopwords\n",
    "        )  \n",
    "    return Counter(palavras)\n",
    "\n",
    "# Contar palavras relevantes nos títulos dos vídeos mais vistos (top 20%)\n",
    "top_videos = df_videos.nlargest(int(len(df_videos) * 0.2), \"views\")\n",
    "palavras_relevantes = contar_palavras_relevantes(top_videos[\"title\"])\n",
    "\n",
    "# Exibir as 10 palavras mais frequentes nos títulos dos vídeos de maior sucesso\n",
    "palavras_relevantes.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palavras irrelevantes (stopwords)\n",
    "stopwords = {\"a\", \"o\", \"os\", \"as\", \"de\", \"da\", \"do\", \"dos\", \"das\", \"em\", \"para\", \n",
    "             \"por\", \"com\", \"um\", \"uma\", \"uns\", \"umas\", \"e\", \"ou\", \"se\", \"que\", \n",
    "             \"é\", \"na\", \"no\", \"nas\", \"nos\", \"como\", \"mais\", \"menos\", \"muito\", \n",
    "             \"pouco\", \"ser\", \"ter\", \"vai\", \"tá\", \"to\", \"vou\", \"sobre\", \"essa\", \"esse\",\n",
    "             \"isso\", \"isso\", \"todo\", \"toda\", \"todos\", \"todas\", \"neste\", \"nesta\", \"nesse\"}\n",
    "\n",
    "# Função para limpar o texto das descrições\n",
    "def limpar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Remover URLs (https:// ou www.)\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "    # Remover hashtags e menções (@usuário)\n",
    "    texto = re.sub(r\"#\\w+|@\\w+\", \"\", texto)\n",
    "    # Remover caracteres especiais que não são palavras\n",
    "    texto = re.sub(r\"[^\\w\\s]\", \"\", texto)\n",
    "    return texto\n",
    "\n",
    "# Função para contar palavras relevantes em descrições\n",
    "def contar_palavras_relevantes_desc(textos):\n",
    "    palavras = []\n",
    "    for texto in textos:\n",
    "        texto_limpo = limpar_texto(texto)\n",
    "        palavras.extend(\n",
    "            word for word in re.findall(r'\\b\\w+\\b', texto_limpo) if word not in stopwords\n",
    "        )\n",
    "    return Counter(palavras)\n",
    "\n",
    "# Contar palavras relevantes nas descrições dos vídeos mais vistos (top 20%)\n",
    "palavras_relevantes_desc = contar_palavras_relevantes_desc(top_videos[\"description\"])\n",
    "\n",
    "# Exibir as 10 palavras mais frequentes nas descrições dos vídeos de maior sucesso\n",
    "palavras_relevantes_desc.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configurar o TF-IDF\n",
    "vectorizer_title = TfidfVectorizer(max_features=100)\n",
    "#vectorizer_desc = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Transformar títulos e descrições\n",
    "title_tfidf = vectorizer_title.fit_transform(df_videos[\"title\"])\n",
    "#desc_tfidf = vectorizer_desc.fit_transform(df_videos[\"description\"])\n",
    "\n",
    "# Converter para DataFrame\n",
    "df_title_tfidf = pd.DataFrame(title_tfidf.toarray(), columns=vectorizer_title.get_feature_names_out())\n",
    "#df_desc_tfidf = pd.DataFrame(desc_tfidf.toarray(), columns=vectorizer_desc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar TF-IDF com o DataFrame principal\n",
    "#df_model = pd.concat([df_videos, df_title_tfidf, df_desc_tfidf], axis=1)\n",
    "df_model = pd.concat([df_videos, df_title_tfidf], axis=1)\n",
    "\n",
    "# Selecionar as features que vamos usar para o modelo\n",
    "features = df_model.drop(columns=[\"video_id\", \"title\", \"description\", \"published_at\", \"duration\", \"views\"])\n",
    "target = df_model[\"views\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Treinar o modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest - MSE: {mse_rf}\")\n",
    "print(f\"Random Forest - R² Score: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"R² Score após normalização: {r2_score(y_test, y_pred_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converter X_train e X_test para arrays NumPy\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Verificar se a conversão funcionou\n",
    "print(type(X_train), X_train.shape)\n",
    "print(type(X_test), X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Criar o modelo XGBoost\n",
    "model_xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost - MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost - R² Score: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definir o modelo base\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definir os hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Rodar o GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring=\"r2\", verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhor modelo encontrado\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Avaliar o modelo otimizado\n",
    "y_pred_rf_best = best_rf.predict(X_test)\n",
    "mse_rf_best = mean_squared_error(y_test, y_pred_rf_best)\n",
    "r2_rf_best = r2_score(y_test, y_pred_rf_best)\n",
    "\n",
    "print(f\"Melhor Random Forest - MSE: {mse_rf_best}\")\n",
    "print(f\"Melhor Random Forest - R² Score: {r2_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = {\n",
    "    \"Linear Regression\": r2,\n",
    "    \"Random Forest\": r2_rf,\n",
    "    \"Random Forest (Tuned)\": r2_rf_best,\n",
    "    \"XGBoost\": r2_xgb\n",
    "}\n",
    "\n",
    "for model, score in models_results.items():\n",
    "    print(f\"{model}: R² Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o RMSE para cada modelo\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "rmse_rf_best = np.sqrt(mse_rf_best)\n",
    "\n",
    "print(f\"XGBoost - RMSE: {rmse_xgb}\")\n",
    "print(f\"Random Forest (Otimizado) - RMSE: {rmse_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ajustando hiperparâmetros manualmente\n",
    "model_xgb_tuned = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=8, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "model_xgb_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_xgb_tuned = model_xgb_tuned.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "mse_xgb_tuned = mean_squared_error(y_test, y_pred_xgb_tuned)\n",
    "r2_xgb_tuned = r2_score(y_test, y_pred_xgb_tuned)\n",
    "\n",
    "print(f\"XGBoost (Otimizado) - MSE: {mse_xgb_tuned}\")\n",
    "print(f\"XGBoost (Otimizado) - R² Score: {r2_xgb_tuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "joblib.dump(model_xgb_tuned, \"modelo_xgb_tuned.pkl\")\n",
    "\n",
    "# Salvar os vetorizadores TF-IDF\n",
    "joblib.dump(vectorizer_title, \"vectorizer_title.pkl\")\n",
    "#joblib.dump(vectorizer_desc, \"vectorizer_desc.pkl\")\n",
    "\n",
    "print(\"✅ Modelo e vetorizadores salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Se X_train for um array NumPy, converta de volta para DataFrame\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    X_train = pd.DataFrame(X_train, columns=features.columns)\n",
    "\n",
    "# Salvar as colunas\n",
    "joblib.dump(X_train.columns.tolist(), \"X_train_columns.pkl\")\n",
    "print(\"✅ Colunas do X_train salvas!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
